{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815c0f9f",
   "metadata": {},
   "source": [
    "<h1> Chord Accompaniment using RL - Agent Training </h1>\n",
    "\n",
    "\\\n",
    "**POC:** Nina Rauscher (nr2861@columbia.edu)\n",
    "\n",
    "\\\n",
    "In this notebook, we will **train an RL agent** to **create a chord accompaniment for a given melody**. \n",
    "\n",
    "Previously, we built a chord transition matrix based on a subset of Beethoven sonatas (saved in the *transition_matrix.csv* file) to serve as empirical reward for our RL problem.\n",
    "\n",
    "\\\n",
    "**Our main steps are:**\n",
    "1. Create the environment using Open AI Gym\n",
    "2. Define a PPO class corresponding to the selected algorithm\n",
    "3. Prepare for training by building the appropriate wrappers to visualize the rewards on Weights and Biases\n",
    "4. Train the PPO model\n",
    "5. Evaluate the performance of our model vs a baseline model (random actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05f2c2",
   "metadata": {},
   "source": [
    "<h2> Necessary imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f06fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd16ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Gym - useful to create the environment and step function\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch to design the PPO algorithm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9588785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance visualization libraries\n",
    "!pip install wandb -qqq\n",
    "import os\n",
    "import wandb\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\" # Wait 300s before timeout\n",
    "\n",
    "# Login to weights and biases and initialization of the session (run)\n",
    "wandb.login()\n",
    "run=wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a72b79",
   "metadata": {},
   "source": [
    "<h2> Step 1: Environment creation with Open AI Gym </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8cddc",
   "metadata": {},
   "source": [
    "We consider a setting of discrete states and actions:\n",
    "* Each **state** is represented by a **tuple (Chord, Melody note)**\n",
    "* The agent's **action** is to **select the next chord**. \n",
    "\n",
    "As we restricted the potential chords to major, minor, dominant 7th, or minor 7th, there are **576 states and 48 discrete actions**, which remains reasonable in terms of calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfe5af",
   "metadata": {},
   "source": [
    "<img src=\"Chord%20accompaniment%20loop%20schema.png\" alt=\"Chord accompaniment loop schema\" style=\"width: 60%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963e623",
   "metadata": {},
   "source": [
    "When it comes to the most important part of the environment, which is the reward structure, we decided to consider an **empirical reward** that reflects how good the action is based on **frequencies of chord transitions** we’ve collected from musical datasets, and a **legal reward** that captures the consistency of the action with respect to **music theory rules**.\n",
    "\n",
    "$$\n",
    "r^{\\text{emp}}_t \n",
    "=\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "  &\\text{If $\\hat{\\mathbb{P}}$($C_t$, $M_t$) > 0,} \\quad \\text{MaxiReward}*\\text{$\\hat{\\mathbb{P}}$($C_t$, $M_t$)} \\nonumber \\\\\n",
    "  & \\text{Otherwise,} \\quad \\text{MiniReward} \\nonumber\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    &\\text{and}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "r^{\\text{leg}}_t \n",
    "=\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "  &\\text{If rules are respected,} \\quad \\text{BonusReward} \\nonumber \\\\\n",
    "  &\\text{Otherwise,} \\quad \\text{NegReward} \\nonumber\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "To compute the empirical reward for a given state and action, we need to retrieve the data from our analysis of famous songs datasets (e.g., a subset of Beethoven sonatas). Let's start by doing that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e441c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_matrix = pd.read_csv('transition_matrix.csv', delimiter = ',', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the format of the matrix by taking a look at its first 10 rows\n",
    "frequencies_matrix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ee9ea",
   "metadata": {},
   "source": [
    "Now that we have all the data we need, we can create the environment by defining the `MusicEnv` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEnv(gym.Env):\n",
    "    def __init__(self, initial_state, melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward):\n",
    "      super(MusicEnv, self).__init__()\n",
    "      self.obs_space = gym.spaces.MultiDiscrete([48, 12]) # 48 chords x 12 melody notes\n",
    "      self.act_space = gym.spaces.Discrete(48) # we choose to respect this order M - m - 7M - 7m\n",
    "      self.start_pos = initial_state\n",
    "      self.current_state = initial_state\n",
    "\n",
    "\n",
    "      self.melody = melody_notes # Only the downbeats of a given melody\n",
    "      self.current_melody_index = 0\n",
    "      self.horizon = len(melody_notes)-1 # The number of actions to take is equal to the length of the input melody - 1\n",
    "\n",
    "\n",
    "      self.frequencies = frequencies_matrix # Matrix from the empirical chord transition frequencies\n",
    "      self.MaximuxReward = MaxiReward # Hyperparameter : Reward if the transition was already seen on data\n",
    "      self.MinimumReward = MiniReward # Hyperparameter : Reward to be given if the transition was not seen on data\n",
    "      self.BonusReward = BonusReward # Hyperparameter: Bonus for respecting the rules\n",
    "      self.NegReward = NegReward # Hyperparameter: Negative reward for not respecting the rules\n",
    "\n",
    "\n",
    "      self.CHORDS_to_INT = {'C':0, 'C#':1, 'D':2, 'D#':3, 'E':4,'F':5,'F#':6,'G':7, 'G#':8, 'A':9, 'A#':10, 'B':11}\n",
    "      self.INT_to_CHORDS = {0:'C', 1:'C#', 2:'D', 3:'D#', 4:'E', 5:'F', 6:'F#', 7:'G', 8:'G#', 9:'A', 10:'A#', 11:'B'}\n",
    "      self.TYPES_to_INT = {'M':0, 'm':1, 'M7':2, 'm7':3}\n",
    "      self.INT_to_TYPES = {0:'M', 1:'m', 2:'M7', 3:'m7'}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "      self.current_chord = self.start_pos\n",
    "      self.current_melody_index = 0\n",
    "      return self.current_chord\n",
    "\n",
    "    def _integer_to_chord(self, integer):\n",
    "      \"\"\"\n",
    "      Returns chord, type given an integer between 0 and 47\n",
    "      \"\"\"\n",
    "      q, r = integer//12, integer%12\n",
    "      return self.INT_to_CHORDS[r], self.INT_to_TYPES[q]\n",
    "\n",
    "    def _chord_to_integer(self, chord, chord_type):\n",
    "      \"\"\"\n",
    "      Returns an integer between 0 and 47 given a chord and chord_type\n",
    "      \"\"\"\n",
    "      return self.TYPES_to_INT[chord_type]*12 + self.CHORDS_to_INT[chord]\n",
    "\n",
    "    def _melody_note_to_integer(self, melody_note):\n",
    "      \"\"\"\n",
    "      Returns an integer between 0 and 11 given a melody note\n",
    "      \"\"\"\n",
    "      return self._chord_to_integer(melody_note, 'M')\n",
    "\n",
    "    def _possible_legal_transitions(self, chord, chord_type='M'):\n",
    "      \"\"\"\n",
    "      chord_type could take values in {'M', 'm', 'M7', 'm7'}\n",
    "      Returns legal melody notes to be played with the chord `chord`\n",
    "      \"\"\"\n",
    "      x = self.CHORDS_to_INT[chord]\n",
    "      if chord_type=='M': # (X, X+4, (X+5), X+7, X+9) are legal\n",
    "          legal_melodies = [c for c, idx in self.CHORDS_to_INT.items() if idx in {x, (x+4)%12, (x+5)%12, (x+7)%12, (x+9)%12}]\n",
    "      if chord_type=='m': # (X, X+3, (X+5), X+7, X+8) are legal\n",
    "          legal_melodies = [c for c, idx in self.CHORDS_to_INT.items() if idx in {x, (x+3)%12, (x+5)%12, (x+7)%12, (x+8)%12}]\n",
    "      if chord_type=='M7': # (X, X+4, X+7, X+10) are legal\n",
    "          legal_melodies = [c for c, idx in self.CHORDS_to_INT.items() if idx in {x, (x+4)%12, (x+7)%12, (x+10)%12}]\n",
    "      if chord_type=='m7': # (X, X+3, X+7, X+10) are legal\n",
    "          legal_melodies = [c for c, idx in self.CHORDS_to_INT.items() if idx in {x, (x+3)%12, (x+7)%12, (x+10)%12}]\n",
    "      return legal_melodies\n",
    "\n",
    "    def _is_legal_transition(self, chord, chord_type, melody_note):\n",
    "      legal_transitions = self._possible_legal_transitions(chord, chord_type)\n",
    "      return (melody_note in legal_transitions)\n",
    "\n",
    "    def step(self, action):\n",
    "      \"action is an integer between 0-47\"\n",
    "\n",
    "      if self.current_melody_index >= self.horizon:\n",
    "          print(f\"Action taken after episode end: Action: {action}, State: {self.current_state}\")\n",
    "          # Episode has ended, no further actions should be taken\n",
    "          return np.array([0, 0]), 0, True, {}  # Example default values\n",
    "\n",
    "      next_chord, next_chord_type = self._integer_to_chord(action)\n",
    "      curr_chord, curr_chord_type = self._integer_to_chord(self.current_state[0])\n",
    "      empirical_freq = self.frequencies.loc[curr_chord+curr_chord_type, next_chord+next_chord_type]\n",
    "      # print(\"Empirical_freq:\", empirical_freq)\n",
    "\n",
    "      if (empirical_freq > 0) :\n",
    "          empirical_reward = self.MaximuxReward * empirical_freq\n",
    "      else:\n",
    "          empirical_reward = self.MinimumReward\n",
    "\n",
    "      # Check if the next index is within the range of the melody list\n",
    "      legal_reward = 0\n",
    "      if self.current_melody_index + 1 < self.horizon:\n",
    "            next_melody_note = self.melody[self.current_melody_index + 1]\n",
    "\n",
    "            is_legal_transition = self._is_legal_transition(next_chord, next_chord_type, next_melody_note)\n",
    "            legal_reward = self.BonusReward * is_legal_transition + self.NegReward * (1 - is_legal_transition)\n",
    "            next_melody_int = self._melody_note_to_integer(next_melody_note)\n",
    "\n",
    "      # Give extra reward if we are in the last action and it is equal to the first chord\n",
    "      if (self.current_melody_index + 1 == self.horizon):\n",
    "        if (action == self.start_pos[0]):\n",
    "            legal_reward += self.BonusReward*3\n",
    "        next_melody_int = self._melody_note_to_integer(self.melody[-1])\n",
    "\n",
    "      # Debug information for early steps of an episode\n",
    "      # if self.current_melody_index < 5:\n",
    "      #       print(f\"Early step debugging: Action: {action}, Current State: {self.current_state}\")\n",
    "\n",
    "      # Update the current state\n",
    "      self.current_state = np.array([action, next_melody_int])\n",
    "      reward = empirical_reward + legal_reward\n",
    "\n",
    "      self.current_melody_index += 1\n",
    "      done = (self.current_melody_index +1 >= self.horizon)\n",
    "\n",
    "\n",
    "      # Debug: Print the current state, action, and next state\n",
    "\n",
    "      # print(f\"Current state: {self.current_state}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "      return self.current_state, reward, done, {}\n",
    "\n",
    "      #except Exception as e:\n",
    "      #    print(f\"Error in step method: {e}\")\n",
    "          # Return a default tuple on error\n",
    "      #    return np.array([0, 0]), 0, True, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf25c7f",
   "metadata": {},
   "source": [
    "<h2> Step 2: Define a PPO class corresponding to the selected algorithm </h2>\n",
    "\n",
    "Now that we have created the environment, we need to implement an algorithm whose objective will be to choose the best action to take at each step.\n",
    "\n",
    "\\\n",
    "Our chosen algorithm, **the proximal policy optimization (PPO) with a clipped objective**, has numerous advantages. \n",
    "\n",
    "First, PPO is **simple to implement**, with no requirement of analytical results or second-order derivatives. Instead, it relies on updates via SGD algorithms, namely ADAM in this case. \n",
    "\n",
    "Besides, PPO exhibits far more **stable training** than a vanilla policy gradient method (such as\n",
    "REINFORCE), since it discourages large policy updates by clipping the surrogate advantage. \n",
    "\n",
    "Moreover, the **complexity of the spaces can be increased** without requiring any modifications to our algorithm. This was an important point at the beginning of our work as we were likely to modify the definition of our state and action spaces.\n",
    "\n",
    "\\\n",
    "As literature review has shown the **relevancy of actor-critic methods** for our problem, we incorporated one in our PPO implementation. Both the actor and the critic are structured as **deep neural networks with three hidden layers**, each containing 64 units:\n",
    "\n",
    "The **actor** takes in the current state as input and **outputs the probability distribution over possible actions**, while the **critic** estimates the value of the current state and **predicts the total expected reward**, which helps to guide the actor’s decisions.\n",
    "\n",
    "\\\n",
    "Finally, to ensure that our results are satisfying, we have implemented a **baseline model** where actions are selected randomly from a **uniform random distribution** (see the results later).\n",
    "\n",
    "\\\n",
    "ℹ️ More information on the theory can be found in the README file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "\n",
    "  def __init__(self, env, obs_dim, act_dim, lr=0.005):\n",
    "\n",
    "        self.env = env\n",
    "        self.obs_dim = env.obs_space.shape[0]\n",
    "        self.act_dim = env.act_space.n\n",
    "        self.lr = lr\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, self.act_dim) # output: probability distribution over all possible actions\n",
    "        )\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1) # output: value function estimation\n",
    "        )\n",
    "\n",
    "        self._init_hyperparameters()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.actor.parameters()},\n",
    "            {'params': self.critic.parameters()}\n",
    "        ], lr=self.lr)\n",
    "\n",
    "\n",
    "  def _init_hyperparameters(self):\n",
    "        self.steps_per_batch = 4800\n",
    "        self.max_steps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.n_updates_per_iteration = 5\n",
    "        self.clip = 0.2\n",
    "\n",
    "  def get_action(self, obs):\n",
    "        # If obs is already a tensor, clone and detach it. Otherwise, create a new tensor.\n",
    "        if isinstance(obs, torch.Tensor):\n",
    "          obs = obs.clone().detach()\n",
    "        else:\n",
    "          obs = torch.tensor(obs, dtype=torch.float)\n",
    "        action_probs = F.softmax(self.actor(obs), dim=-1)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        if action is None or log_prob is None:\n",
    "          print(f\"Invalid action or log_prob: action={action}, log_prob={log_prob}, obs={obs}\")\n",
    "\n",
    "        return action.item(), log_prob\n",
    "\n",
    "  def get_action_random(self, obs):\n",
    "        action_probs = torch.tensor([1/self.act_dim] * self.act_dim)\n",
    "\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        if action is None or log_prob is None:\n",
    "          print(f\"Invalid action or log_prob: action={action}, log_prob={log_prob}, obs={obs}\")\n",
    "\n",
    "        return action.item(), log_prob\n",
    "\n",
    "  def evaluate(self, batch_obs, batch_acts):\n",
    "        action_probs = F.softmax(self.actor(batch_obs), dim=-1)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(batch_acts)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(batch_obs).squeeze()\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "  def learn(self, max_steps):\n",
    "        step = 0\n",
    "\n",
    "        while step < max_steps:\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "\n",
    "            step += np.sum(batch_lens)\n",
    "\n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                curr_log_probs, V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "                V = V.squeeze()\n",
    "\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs) # importance sampling ratio\n",
    "                A_k = batch_rtgs - V.detach() # advantage\n",
    "                surr1 = ratios * A_k # surrogate advantage 1\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k # surrogate advantage 2\n",
    "\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                critic_loss = torch.nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update steps\n",
    "            step += np.sum(batch_lens)\n",
    "\n",
    "  def rollout(self):\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews  =[]\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "\n",
    "        step = 0\n",
    "        while step < self.steps_per_batch:\n",
    "\n",
    "          ep_rews = []\n",
    "          obs = self.env.reset()\n",
    "          done = False\n",
    "\n",
    "          for ep_t in range(self.max_steps_per_episode):\n",
    "            step += 1\n",
    "            batch_obs.append(obs)\n",
    "\n",
    "            action, log_prob = self.get_action(obs)\n",
    "            obs, rew, done, _ = self.env.step(action)\n",
    "\n",
    "            ep_rews.append(rew)\n",
    "            batch_acts.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            if done:\n",
    "              break\n",
    "\n",
    "          batch_lens.append(ep_t + 1)\n",
    "          batch_rews.append(ep_rews)\n",
    "\n",
    "        batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "\n",
    "        return  batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "\n",
    "  def compute_rtgs(self, batch_rews):\n",
    "\n",
    "        \"\"\" Computes the \"return-to-go\" for each timestep in each episode\"\"\"\n",
    "\n",
    "        batch_rtgs = []\n",
    "\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "          discounted_reward = 0\n",
    "\n",
    "          for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * self.gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "        batch_rtgs = torch.tensor(batch_rtgs,  dtype=torch.float)\n",
    "\n",
    "        return batch_rtgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7045e",
   "metadata": {},
   "source": [
    "<h2> Step 3: Prepare for training by building the appropriate wrappers to visualize the rewards on Weights and Biases </h2>\n",
    "\n",
    "For proper accounting of rewards while the agent learns, we build a wrapper around `env.step()` and `env.reset()`. In an episode, every time you take an action the reward will be appended to the episode reward, and whenever the environment is reset (at the end of an epsiode), the episode reward is reset to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094161a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, ppo_model, num_episodes, max_steps_per_episode, config):\n",
    "    wandb.init(project=\"ppo-training\", config=config)\n",
    "    memory = Memory()\n",
    "    rrecord = []  # Record of rewards for each episode\n",
    "    fixedWindow = 10\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Convert state to a NumPy array if it's not already one, then to tensor\n",
    "            state_array = np.array(state) if isinstance(state, list) or isinstance(state, tuple) else state\n",
    "            state_tensor = torch.from_numpy(state_array).float().unsqueeze(0)\n",
    "\n",
    "            # Get action and log probability from PPO policy\n",
    "            action, log_prob = ppo_model.get_action(state_tensor)\n",
    "\n",
    "            # Debug: Print the output of env.step(action)\n",
    "            if env.step(action) is None:\n",
    "                print(f\"Episode: {episode}, Step: {step}, Received None action. State tensor: {state_tensor}\")\n",
    "                continue  # Skip this step or handle it as needed\n",
    "\n",
    "            try:\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception in env.step(): {e}, Episode: {episode}, Step: {step}, Action: {action}, State: {state}\")\n",
    "                break\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Save data in memory\n",
    "            memory.states.append(state_tensor)\n",
    "            memory.actions.append(torch.tensor([action]))\n",
    "            memory.logprobs.append(log_prob)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Update PPO policy after each episode\n",
    "        total_steps = 4800  # Number of steps to collect before each update, can be tuned further\n",
    "        ppo_model.learn(total_steps)\n",
    "\n",
    "        rrecord.append(episode_reward)\n",
    "\n",
    "        # Printing functions for debugging purposes. Feel free to add more if necessary\n",
    "        if episode % 10 == 0 and episode != 0:\n",
    "            print(f\"Episode: {episode}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "            print('episode {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
    "\n",
    "        # Log episode reward\n",
    "\n",
    "        # Calculate average of the last 'fixedWindow' elements. If less than 'fixedWindow' episodes, average of all so far\n",
    "        movingAverage = np.mean(rrecord[-fixedWindow:]) if len(rrecord) >= fixedWindow else np.mean(rrecord)\n",
    "\n",
    "        wandb.log({\"training reward\": episode_reward, \"training reward moving average\": movingAverage})\n",
    "\n",
    "\n",
    "    wandb.run.summary[\"number of training episodes\"] = num_episodes\n",
    "\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c88bcb",
   "metadata": {},
   "source": [
    "<h2> Step 4: Train the PPO model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8799ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create training melody notes first \n",
    "text = \"\"\"\n",
    "Ab Ab F F\n",
    "G C E G\n",
    "Bb Bb G G\n",
    "Ab Ab F F\n",
    "Bb Bb G G\n",
    "C C C C\n",
    "Ab Eb Db Db\n",
    "Db Bb Ab Ab\n",
    "G Eb Db Db\n",
    "Ab Db C C\n",
    "F F F F\n",
    "G Eb Db C\n",
    "C Bb Bb Ab\n",
    "G Eb Db C\n",
    "C Bb Bb Ab\n",
    "G G G E\n",
    "Eb Db Bb G\n",
    "E E Ab E\n",
    "Eb Db Bb G\n",
    "E E Ab E\n",
    "Eb Db Bb G\n",
    "C Bb G G\n",
    "Ab Db F Ab\n",
    "Eb Eb F Ab\n",
    "Eb Eb Db E\n",
    "Ab Ab Ab Ab\n",
    "\"\"\"\n",
    "\n",
    "# Define replacements\n",
    "replacements = {'Db': 'C#', 'Bb': 'A#', 'Eb': 'D#', 'Ab': 'G#'}\n",
    "\n",
    "# Apply replacements and create a list\n",
    "train_data = [replacements.get(chord, chord) for line in text.strip().split('\\n') for chord in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "\n",
    "# PPO hyperparameters and training configuration\n",
    "config = {\n",
    "    \"learning_rate\": 0.008,\n",
    "    \"gamma\": 0.95,\n",
    "    \"num_episodes\": 100,\n",
    "    \"max_steps_per_episode\": 100,\n",
    "    # ... any other hyperparameters or settings ...\n",
    "}\n",
    "\n",
    "initial_state = np.array([5,1]) # Equivalent to (chord, chord_type) = Fm\n",
    "\n",
    "melody_notes = train_data\n",
    "\n",
    "# Rewards hyperparameters (potential to tune them more but these values already lead to pretty good results)\n",
    "MaxiReward = 100\n",
    "MiniReward = -5\n",
    "BonusReward = 15\n",
    "NegReward = -10\n",
    "\n",
    "# Initialize the environment\n",
    "env = MusicEnv(initial_state, melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward)  # Initialize with appropriate parameters\n",
    "\n",
    "# PPO hyperparameters\n",
    "obs_dim = env.obs_space.shape[0]\n",
    "act_dim = env.act_space.n\n",
    "ppo_model = PPO(env, obs_dim, act_dim, lr=config['learning_rate'])\n",
    "\n",
    "# Start training\n",
    "train_ppo(env, ppo_model, config[\"num_episodes\"], config[\"max_steps_per_episode\"], config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f61ca6",
   "metadata": {},
   "source": [
    "<h2> Step 5: Evaluate the performance of our model vs baseline (random actions) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f801f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ppo(ppo_model, initial_state, melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward, max_steps_per_melody, num_episodes=100, baseline=True):\n",
    "    rList = []  # List to store rewards of each episode\n",
    "    movingAverageArray = []  # List to store moving average\n",
    "    best_reward = -float('inf')  # Initialize best_reward with a very low value\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env = MusicEnv(initial_state, melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward)\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        generated_chords = []  # List to store generated chords for this episode\n",
    "\n",
    "        for step in range(max_steps_per_melody):\n",
    "            state_array = np.array(state) if isinstance(state, list) else state\n",
    "            state_tensor = torch.from_numpy(state_array).float().unsqueeze(0)\n",
    "            if baseline == True:\n",
    "              action, log_prob = ppo_model.get_action_random(state_tensor) # Baseline model\n",
    "            else:\n",
    "              action, log_prob = ppo_model.get_action(state_tensor)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            chord, chord_type = env._integer_to_chord(action)  # Convert action to chord\n",
    "            generated_chords.append(chord + chord_type)  # Combine chord and type\n",
    "\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Check if this episode has the highest reward so far\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_chords = generated_chords.copy()\n",
    "\n",
    "        # Log episode reward per 10 episodes and compute moving average\n",
    "        rList.append(episode_reward)\n",
    "        movingAverage = np.mean(rList[-10:]) if len(rList) >= 10 else np.mean(rList)\n",
    "        movingAverageArray.append(movingAverage)\n",
    "\n",
    "        # Log to Wandb\n",
    "        if baseline == True:\n",
    "          wandb.log({\"evaluation reward_baseline\": episode_reward, \"evaluation reward moving average_baseline\": movingAverage, \"evaluation episode_baseline\": episode})\n",
    "\n",
    "        else:\n",
    "          wandb.log({\"evaluation reward\": episode_reward, \"evaluation reward moving average\": movingAverage, \"evaluation episode\": episode})\n",
    "\n",
    "    # Calculate the final score\n",
    "    score = max(movingAverageArray[-10:]) if len(movingAverageArray) >= 10 else max(movingAverageArray)\n",
    "    wandb.run.summary[\"score\"] = score\n",
    "\n",
    "    return best_chords, best_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ccb914",
   "metadata": {},
   "source": [
    "<h3> 5.1. Evaluate a baseline model taking random actions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "# Baseline model\n",
    "initial_state = [5, 1]  # Define your initial state: Fm\n",
    "test_melody_notes = ['D', 'D', 'C', 'C', 'A', 'A', 'A', 'F', 'D', 'D', 'C', 'C', 'G', 'G', 'A', 'A']\n",
    "max_steps_per_melody = len(test_melody_notes)\n",
    "\n",
    "generated_chords, best_reward = evaluate_ppo(ppo_model, initial_state, test_melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward, max_steps_per_melody, num_episodes=100, baseline=True)\n",
    "print(\"Generated chords:\", generated_chords, \"with highest average reward = \", best_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d9286",
   "metadata": {},
   "source": [
    "<h3> 5.2. Evaluate our PPO model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5742f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "# Our model\n",
    "initial_state = [5, 1]  # Define your initial state: Fm\n",
    "test_melody_notes = ['D', 'D', 'C', 'C', 'A', 'A', 'A', 'F', 'D', 'D', 'C', 'C', 'G', 'G', 'A', 'A']\n",
    "max_steps_per_melody = len(test_melody_notes)\n",
    "\n",
    "generated_chords, best_reward = evaluate_ppo(ppo_model, initial_state, test_melody_notes, frequencies_matrix, MaxiReward, MiniReward, BonusReward, NegReward, max_steps_per_melody, num_episodes=100, baseline=False)\n",
    "print(\"Generated chords:\", generated_chords, \"with highest average reward = \", best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b01ea",
   "metadata": {},
   "source": [
    "Once the `ppo_model` is tuned, you can obtain the generated chords for any given melody notes and initial chord using the `evaluate_ppo` method.\n",
    "\n",
    "We thus decided to apply it to 3 melodies to assess the performance as human listeners. The results are available in the README file along with the corresponding audios and more detailed explanations on our training process.\n",
    "\n",
    "If after reading the README file you have any questions, feel free to reach out at nr2861@columbia.edu ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b041214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
